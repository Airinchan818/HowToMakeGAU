{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Here I want share How to Make  GAU (Gating Attention Units) Layers\n",
        "#for everyone who wanna learn create more light Transformers  "
      ],
      "metadata": {
        "id": "tj3cbnMm3ThS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7qD-RVYurAJv"
      },
      "outputs": [],
      "source": [
        "import littlelearn as ll\n",
        "from littlelearn import DeepLearning as dl\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# With LittleLearn\n",
        "\n"
      ],
      "metadata": {
        "id": "bOqHJ0NB3sGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "to be honest GAU is implemented at layers API, but here i want sharing how we can make it by manual.\n"
      ],
      "metadata": {
        "id": "ME-SQD7e3yQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layers = dl.layers.GAU(32,use_causalmask=True)\n",
        "test = ll.rand(1,1,32)\n",
        "layers(test,test,test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEI0Zbmc3rOt",
        "outputId": "58e707e1-6181-4037-a82c-e0cbab88c9f9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tensor(shape=(1, 1, 32), dtype=<class 'jax.numpy.float32'>, device=cpu, requires_grad=False)\n",
              "[[[-0.21995004  0.11773483  0.06039233 -0.06927146  0.19805276\n",
              "    0.2159711   0.11858805  0.0735042  -0.37021443  0.23604305\n",
              "   -0.08045566 -0.02607459  0.23318562  0.17026182 -0.01790002\n",
              "    0.1831313   0.01150634 -0.10715868 -0.04895158  0.04067403\n",
              "    0.06841192 -0.10092467 -0.2370058   0.05023849 -0.03271296\n",
              "    0.07916956  0.3122727  -0.05678637  0.12025668 -0.06060823\n",
              "    0.02510053 -0.06312887]]]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "y0ofjt3B5Swz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Oke lets build Manual GAU"
      ],
      "metadata": {
        "id": "FC1lQCPE4Z1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LLGAU (dl.layers.Component) :\n",
        "  def __init__ (self,embed_dim) :\n",
        "    super().__init__()\n",
        "    std = math.sqrt(2/(embed_dim * 2))\n",
        "    self.wq = dl.layers.Parameter(\n",
        "        tensor=ll.normal(mean=0,std=std,shape=(embed_dim,embed_dim))\n",
        "    )\n",
        "    self.wk = dl.layers.Parameter(\n",
        "        tensor=ll.normal(mean=0,std=std,shape=(embed_dim,embed_dim))\n",
        "    )\n",
        "    self.wv = dl.layers.Parameter(\n",
        "        tensor=ll.normal(mean=0,std=std,shape=(embed_dim,embed_dim))\n",
        "    )\n",
        "    self.wgate = dl.layers.Parameter(\n",
        "        tensor=ll.normal(mean=0,std=std,shape=(embed_dim,embed_dim))\n",
        "    )\n",
        "    self.wout = dl.layers.Parameter(\n",
        "        tensor=ll.normal(mean=0,std=std,shape=(embed_dim,embed_dim))\n",
        "    )\n",
        "\n",
        "  def count_score (self,q,k,v) :\n",
        "    score = ll.matmul(q,k,transpose_b=True) / k.shape[-1] ** 0.5\n",
        "    score = dl.activations.softplus(score)\n",
        "    out = ll.matmul(score,v)\n",
        "    return out\n",
        "\n",
        "  def forwardpass(self,q,k,v) :\n",
        "    b,s,d = q.shape\n",
        "    Q = ll.matmul(q,self.wq)\n",
        "    K = ll.matmul(k,self.wk)\n",
        "    V = ll.matmul(v,self.wv)\n",
        "\n",
        "    gating = ll.matmul(q,self.wgate)\n",
        "    gating = dl.activations.sigmoid(gating)\n",
        "    out = self.count_score(Q,K,V)\n",
        "    out = out * gating\n",
        "    out = ll.matmul(out,self.wout)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "qacV_oYb4lca"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "testing"
      ],
      "metadata": {
        "id": "Nyh2XyxM64XC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = ll.randn((1,1,32),requires_grad=True)\n",
        "layers = LLGAU(32)\n",
        "layers.train(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVHAehJn63FU",
        "outputId": "e81aa05e-9bf7-4843-d7db-d44cd3455adb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name : LLGAU status : active || requires_grad : True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = layers(logits,logits,logits)\n",
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK168r5m7HN7",
        "outputId": "13c8f268-9827-41e4-bfcc-c2ad790ef0e1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tensor(shape=(1, 1, 32), dtype=<class 'jax.numpy.float32'>, device=cpu, requires_grad=True)\n",
              "[[[ 3.6267908  -1.3859645  -1.5053034   5.198196   -0.4432551\n",
              "    4.5121636   5.9736137  -6.0510736   1.4292274  -1.6760142\n",
              "   -0.5340683  -0.08145965 -6.4350147   7.6194444  -2.1674361\n",
              "   -0.03600753 -5.4629164  -4.6233478  -1.9080942  -2.636688\n",
              "   -1.2105691   1.2777492  -8.243667    6.5960474   5.071932\n",
              "   -3.317241    5.11329    -5.6657777   1.2207596   3.855531\n",
              "   -1.169121    0.07661773]]]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out.backwardpass()"
      ],
      "metadata": {
        "id": "CdNMGKxf7nTk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.reset_grad()"
      ],
      "metadata": {
        "id": "-7KgZBK47rEM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# With Torch"
      ],
      "metadata": {
        "id": "SwIWeVev8LqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TorchGAU (nn.Module) :\n",
        "  def __init__(self,embed_dim) :\n",
        "    super().__init__()\n",
        "    std = math.sqrt(2/(embed_dim * 2))\n",
        "    self.wq = nn.Parameter(torch.normal(mean=0,std=std,size=(embed_dim,embed_dim)))\n",
        "    self.wk = nn.Parameter(torch.normal(mean=0,std=std,size=(embed_dim,embed_dim)))\n",
        "    self.wv = nn.Parameter(torch.normal(mean=0,std=std,size=(embed_dim,embed_dim)))\n",
        "    self.wgate = nn.Parameter(torch.normal(mean=0,std=std,size=(embed_dim,embed_dim)))\n",
        "    self.wout = nn.Parameter(torch.normal(mean=0,std=std,size=(embed_dim,embed_dim)))\n",
        "\n",
        "  def count_attention (self,q,k,v) :\n",
        "    score = torch.matmul(q,k.transpose(-2,-1)) / k.shape[-1] ** 0.5\n",
        "    score = torch.nn.functional.softplus(score)\n",
        "    out = torch.matmul(score,v)\n",
        "    return out\n",
        "\n",
        "  def forward(self,q,k,v) :\n",
        "    b,s,d = q.shape\n",
        "    Q = torch.matmul(q,self.wq)\n",
        "    K = torch.matmul(k,self.wk)\n",
        "    V = torch.matmul(v,self.wv)\n",
        "\n",
        "    gating = torch.matmul(q,self.wgate)\n",
        "    gating = torch.sigmoid(gating)\n",
        "    out = self.count_attention(Q,K,V)\n",
        "    out = out * gating\n",
        "    out = torch.matmul(out,self.wout)\n",
        "    return out"
      ],
      "metadata": {
        "id": "irIJloQT8LST"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "P5yP3SJM9rRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layers = TorchGAU(32)\n",
        "layers.train(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4sy06xR9HOX",
        "outputId": "ef5053f7-1b6b-43be-9fb7-22287bc65cd9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TorchGAU()"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits = torch.rand(1,1,32)"
      ],
      "metadata": {
        "id": "I89-z9wI9TTc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = layers(logits,logits,logits)\n",
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4ESLP719QS9",
        "outputId": "8b0d80c1-f43c-428c-b7fa-126e9d339773"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0010,  0.0017, -0.0089,  0.0996,  0.2169,  0.2483, -0.2100,\n",
              "          -0.2156, -0.1056, -0.1064, -0.1626, -0.2534, -0.3525, -0.0466,\n",
              "           0.0683,  0.2508,  0.1061, -0.2134, -0.0947, -0.6474, -0.2895,\n",
              "          -0.2039,  0.3118,  0.0417, -0.0400,  0.0580, -0.0066,  0.0071,\n",
              "           0.0354, -0.2313,  0.2615,  0.1075]]], grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With Tensorflow"
      ],
      "metadata": {
        "id": "vrUqs42X9oQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@keras.utils.register_keras_serializable()\n",
        "class TfGAU (keras.layers.Layer) :\n",
        "  def __init__(self,embed_dim) :\n",
        "    super().__init__()\n",
        "    std = math.sqrt(2/(embed_dim * 2))\n",
        "    self.wq = keras.initializers.RandomNormal(mean=0,stddev=std)(shape=(embed_dim,embed_dim))\n",
        "    self.wk = keras.initializers.RandomNormal(mean=0,stddev=std)(shape=(embed_dim,embed_dim))\n",
        "    self.wv = keras.initializers.RandomNormal(mean=0,stddev=std)(shape=(embed_dim,embed_dim))\n",
        "    self.wgate = keras.initializers.RandomNormal(mean=0,stddev=std)(shape=(embed_dim,embed_dim))\n",
        "    self.wout = keras.initializers.RandomNormal(mean=0,stddev=std)(shape=(embed_dim,embed_dim))\n",
        "\n",
        "  def count_attention (self,q,k,v) :\n",
        "    score = tf.matmul(q,k,transpose_b=True) / k.shape[-1] ** 0.5\n",
        "    score = tf.nn.softplus(score)\n",
        "    out = tf.matmul(score,v)\n",
        "    return out\n",
        "\n",
        "  def call (self,q,k,v) :\n",
        "    b,s,d = q.shape\n",
        "    Q = tf.matmul(q,self.wq)\n",
        "    K = tf.matmul(k,self.wk)\n",
        "    V = tf.matmul(v,self.wv)\n",
        "\n",
        "    gating = tf.matmul(q,self.wgate)\n",
        "    gating = tf.nn.sigmoid(gating)\n",
        "    attn = self.count_attention(Q,K,V)\n",
        "    out = attn * gating\n",
        "    out = tf.matmul(out,self.wout)\n",
        "    return out"
      ],
      "metadata": {
        "id": "lvZfilCO9mwF"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "opPy2eky-waF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layers = TfGAU(32)\n",
        "logits = tf.random.normal((1,1,32))"
      ],
      "metadata": {
        "id": "0X5uyWZW9m3D"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layers(logits,logits,logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGb3Sfxi-3-m",
        "outputId": "84d0f7ff-b13d-4676-8189-9291b4b62267"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1, 32), dtype=float32, numpy=\n",
              "array([[[-0.18055338, -0.12918295,  0.9146644 , -0.19612189,\n",
              "         -0.49990502, -0.2993888 , -0.7781618 , -0.14139004,\n",
              "         -0.6453798 ,  0.05248633,  0.5470862 ,  0.13061911,\n",
              "          0.01359482,  0.03959207,  0.16944441, -0.35442755,\n",
              "         -0.24433155,  0.62876767, -0.4541697 ,  0.33135998,\n",
              "          0.8075741 , -0.3688897 ,  0.25799742, -0.08735079,\n",
              "         -0.72536975, -0.50624907,  0.05734975,  0.44088173,\n",
              "         -0.66767085, -0.3967695 , -0.01072117,  0.18601109]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "oke, lest random choice for training testing with Transformers Model"
      ],
      "metadata": {
        "id": "jzmXrGe5_A4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "print({\n",
        "    \"1\" : \"LittleLearn\",\n",
        "    \"2\" : \"Torch\",\n",
        "    \"3\" : \"Tensorflow\"\n",
        "\n",
        "})\n",
        "\n",
        "print(f\"we will training with {random.choice(['1','2','3'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGCWoRTj_H8b",
        "outputId": "832578cb-74f8-40b8-9d25-04476b310ca6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'1': 'LittleLearn', '2': 'Torch', '3': 'Tensorflow'}\n",
            "we will training with 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "oke, lest training with Torch. i will edit torchGAU for make it suppport DecoderOnly Transformers"
      ],
      "metadata": {
        "id": "TNNAH6sH_Zn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TorchGAU (nn.Module) :\n",
        "  def __init__(self,embed_dim) :\n",
        "    super().__init__()\n",
        "    std = math.sqrt(2/(embed_dim * 2))\n",
        "    self.wq = nn.Parameter(torch.normal(mean=0,std=std,size=(embed_dim,embed_dim)))\n",
        "    self.wk = nn.Parameter(torch.normal(mean=0,std=std,size=(embed_dim,embed_dim)))\n",
        "    self.wv = nn.Parameter(torch.normal(mean=0,std=std,size=(embed_dim,embed_dim)))\n",
        "    self.wgate = nn.Parameter(torch.normal(mean=0,std=std,size=(embed_dim,embed_dim)))\n",
        "    self.wout = nn.Parameter(torch.normal(mean=0,std=std,size=(embed_dim,embed_dim)))\n",
        "\n",
        "  def count_attention (self,q,k,v) :\n",
        "    score = torch.matmul(q,k.transpose(-2,-1)) / k.shape[-1] ** 0.5\n",
        "    mask = 1 - torch.tril(torch.ones(score.shape),diagonal=0)\n",
        "    mask = mask * -1e9\n",
        "    score = score + mask\n",
        "    score = torch.nn.functional.softplus(score)\n",
        "    out = torch.matmul(score,v)\n",
        "    return out\n",
        "\n",
        "  def forward(self,q,k,v) :\n",
        "    b,s,d = q.shape\n",
        "    Q = torch.matmul(q,self.wq)\n",
        "    K = torch.matmul(k,self.wk)\n",
        "    V = torch.matmul(v,self.wv)\n",
        "\n",
        "    gating = torch.matmul(q,self.wgate)\n",
        "    gating = torch.sigmoid(gating)\n",
        "    out = self.count_attention(Q,K,V)\n",
        "    out = out * gating\n",
        "    out = torch.matmul(out,self.wout)\n",
        "    return out"
      ],
      "metadata": {
        "id": "HOnNX2HNBpS1"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\"Happy new years everyone, this is a tutorial from me spesial for last night in  2025  . i hope i will get job, and anyone learn for this tutorial can get job to. i hope anyone here can find with who ones precious.. \"]"
      ],
      "metadata": {
        "id": "N0YCYkss_dQ7"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = ll.preprocessing.Tokenizer()\n",
        "tokenizer.fit_on_texts(data)\n",
        "tokenizer.fit_on_texts([\"sos\" , \"eos\"])\n",
        "x_train = tokenizer.texts_to_sequences([\"sos \" + data[0]])\n",
        "y_train = tokenizer.texts_to_sequences([data[0] + \" eos\"])\n"
      ],
      "metadata": {
        "id": "nwJVXaBrAP8T"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = torch.tensor(x_train)\n",
        "y_train = torch.tensor(y_train)"
      ],
      "metadata": {
        "id": "tB8tWIcJA232"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformersBlock (nn.Module) :\n",
        "  def __init__(self,d_model,droprate) :\n",
        "    super().__init__()\n",
        "    self.norm1 = nn.RMSNorm(d_model)\n",
        "    self.norm2 = nn.RMSNorm(d_model)\n",
        "    self.attention = TorchGAU(d_model)\n",
        "    self.ffn = nn.Sequential(\n",
        "        nn.Linear(d_model,4*d_model),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(4*d_model,d_model)\n",
        "    )\n",
        "    self.dropout1 = nn.Dropout(droprate)\n",
        "    self.dropout2 = nn.Dropout(droprate)\n",
        "\n",
        "  def forward(self,x) :\n",
        "    norm1 = self.norm1(x)\n",
        "    attn = self.attention(norm1,norm1,norm1)\n",
        "    attn = self.dropout1(attn)\n",
        "    x = x + attn\n",
        "\n",
        "    norm2 = self.norm2(x)\n",
        "    ffn = self.ffn(norm2)\n",
        "    ffn = self.dropout2(ffn)\n",
        "    x = x + ffn\n",
        "    return x\n",
        "\n",
        "class DecoderOnlyTransformers (nn.Module) :\n",
        "  def __init__(self,vocab_size,d_model,n_layers,droprate,maxpos) :\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size,d_model)\n",
        "    self.layers = nn.ModuleList([\n",
        "        TransformersBlock(d_model,droprate) for _ in range(n_layers)\n",
        "    ])\n",
        "    self.pos_learn = nn.Embedding(maxpos,d_model )\n",
        "    self.linear = nn.Linear(d_model,vocab_size,bias=False)\n",
        "    self.scale = math.sqrt(float(d_model))\n",
        "\n",
        "  def forward(self,x) :\n",
        "    b,s = x.shape\n",
        "    x = self.embedding(x)\n",
        "    x = x * self.scale\n",
        "    pos = torch.arange(s)\n",
        "    pos  = self.pos_learn(pos)\n",
        "    pos = pos.unsqueeze(0)\n",
        "\n",
        "    x = x + pos\n",
        "    for block in self.layers :\n",
        "      x = block(x)\n",
        "\n",
        "    x = self.linear(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "KRjCLNLQAgWV"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DecoderOnlyTransformers(50,32,3,0.1,50)\n",
        "model.train(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Se4eqGY7D_Fd",
        "outputId": "b1a10149-1663-4d19-80be-b44d647a4229"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecoderOnlyTransformers(\n",
              "  (embedding): Embedding(50, 32)\n",
              "  (layers): ModuleList(\n",
              "    (0-2): 3 x TransformersBlock(\n",
              "      (norm1): RMSNorm((32,), eps=None, elementwise_affine=True)\n",
              "      (norm2): RMSNorm((32,), eps=None, elementwise_affine=True)\n",
              "      (attention): TorchGAU()\n",
              "      (ffn): Sequential(\n",
              "        (0): Linear(in_features=32, out_features=128, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=128, out_features=32, bias=True)\n",
              "      )\n",
              "      (dropout1): Dropout(p=0.1, inplace=False)\n",
              "      (dropout2): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (pos_learn): Embedding(50, 32)\n",
              "  (linear): Linear(in_features=32, out_features=50, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optim = torch.optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "_WlutqIrE7FG"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100) :\n",
        "  y_pred = model(x_train)\n",
        "  loss = loss_fn(y_pred.transpose(-2,-1),y_train)\n",
        "  optim.zero_grad()\n",
        "  loss.backward()\n",
        "  optim.step()\n",
        "\n",
        "  print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2S9dj6WFdH8",
        "outputId": "8c1901a1-fc6b-4731-c8f6-9c425468c113"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.024616535753011703\n",
            "0.09218788146972656\n",
            "0.028056371957063675\n",
            "0.03951152041554451\n",
            "0.04926704242825508\n",
            "0.017196714878082275\n",
            "0.015601158142089844\n",
            "0.03552627190947533\n",
            "0.012745165266096592\n",
            "0.018112873658537865\n",
            "0.023993780836462975\n",
            "0.05996466800570488\n",
            "0.04168415442109108\n",
            "0.06342701613903046\n",
            "0.02466748282313347\n",
            "0.05139461159706116\n",
            "0.018816890195012093\n",
            "0.05477495491504669\n",
            "0.042482130229473114\n",
            "0.029024329036474228\n",
            "0.022892311215400696\n",
            "0.014082225039601326\n",
            "0.017684251070022583\n",
            "0.008813712745904922\n",
            "0.016623396426439285\n",
            "0.03588951751589775\n",
            "0.01944906823337078\n",
            "0.01593070477247238\n",
            "0.019225722178816795\n",
            "0.029220733791589737\n",
            "0.011034054681658745\n",
            "0.014312622137367725\n",
            "0.010603860020637512\n",
            "0.012488635256886482\n",
            "0.011321105062961578\n",
            "0.011183390393853188\n",
            "0.012063626199960709\n",
            "0.01202724315226078\n",
            "0.014223952777683735\n",
            "0.00986446812748909\n",
            "0.01321214996278286\n",
            "0.014996462501585484\n",
            "0.015858441591262817\n",
            "0.015854496508836746\n",
            "0.011062695644795895\n",
            "0.019584642723202705\n",
            "0.010497672483325005\n",
            "0.013602307066321373\n",
            "0.00724331708624959\n",
            "0.008830348029732704\n",
            "0.010200012475252151\n",
            "0.012828410603106022\n",
            "0.008100047707557678\n",
            "0.01437672320753336\n",
            "0.014499678276479244\n",
            "0.01762327551841736\n",
            "0.012731734663248062\n",
            "0.007508499547839165\n",
            "0.09840738028287888\n",
            "0.007051245309412479\n",
            "0.013778194785118103\n",
            "0.011284250766038895\n",
            "0.008935052901506424\n",
            "0.00785237830132246\n",
            "0.016379818320274353\n",
            "0.009448830038309097\n",
            "0.00687374547123909\n",
            "0.007994333282113075\n",
            "0.0056725917384028435\n",
            "0.009676960296928883\n",
            "0.004994579590857029\n",
            "0.015608352608978748\n",
            "0.008158928714692593\n",
            "0.007895335555076599\n",
            "0.010394260287284851\n",
            "0.01528715156018734\n",
            "0.012759283185005188\n",
            "0.011091821826994419\n",
            "0.02640867419540882\n",
            "0.009165603667497635\n",
            "0.006219861097633839\n",
            "0.007637347094714642\n",
            "0.009649371728301048\n",
            "0.005294966045767069\n",
            "0.009395361877977848\n",
            "0.006266927812248468\n",
            "0.008785253390669823\n",
            "0.004988953936845064\n",
            "0.019399089738726616\n",
            "0.006392608396708965\n",
            "0.014579039998352528\n",
            "0.006212861742824316\n",
            "0.012362085282802582\n",
            "0.006410161964595318\n",
            "0.007090856786817312\n",
            "0.010175525210797787\n",
            "0.005927440244704485\n",
            "0.057315677404403687\n",
            "0.008053025230765343\n",
            "0.009574436582624912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPwu1Om_JApF",
        "outputId": "898c788a-a86c-499f-8751-4bbcc5689ccb"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecoderOnlyTransformers(\n",
              "  (embedding): Embedding(50, 32)\n",
              "  (layers): ModuleList(\n",
              "    (0-2): 3 x TransformersBlock(\n",
              "      (norm1): RMSNorm((32,), eps=None, elementwise_affine=True)\n",
              "      (norm2): RMSNorm((32,), eps=None, elementwise_affine=True)\n",
              "      (attention): TorchGAU()\n",
              "      (ffn): Sequential(\n",
              "        (0): Linear(in_features=32, out_features=128, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=128, out_features=32, bias=True)\n",
              "      )\n",
              "      (dropout1): Dropout(p=0.1, inplace=False)\n",
              "      (dropout2): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (pos_learn): Embedding(50, 32)\n",
              "  (linear): Linear(in_features=32, out_features=50, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decod(texts):\n",
        "    r = texts\n",
        "    seq = tokenizer.texts_to_sequences([texts])[0]\n",
        "\n",
        "\n",
        "    input_ = torch.tensor([seq], dtype=torch.long)\n",
        "\n",
        "\n",
        "    start = len(seq)\n",
        "\n",
        "    for _ in range(start, 43):\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_)\n",
        "\n",
        "\n",
        "        logits = logits[:, -1, :]\n",
        "        logits = torch.softmax(logits, dim=-1)\n",
        "\n",
        "        token = torch.multinomial(logits, num_samples=1)\n",
        "        token_id = token.item()\n",
        "\n",
        "\n",
        "        r += \" \" + tokenizer.index_word.get(token_id, \"\")\n",
        "\n",
        "\n",
        "        input_ = torch.cat([input_, token], dim=1)\n",
        "    return r\n"
      ],
      "metadata": {
        "id": "ov9K7hbbFoL9"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5TGbRmD9I_5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decod(\"Happy new years\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "0hcOOPM-HT8W",
        "outputId": "853bfe37-93ca-42a5-b16f-d4de706a6967"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Happy new years everyone this is a tutorial from me spesial for last night in 2025 i hope i will get job and anyone here can find anyone here anyone here job to i hope anyone here can find with who ones precious'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZN4nP6eMH9sl",
        "outputId": "9b8e557e-baff-4c2a-c416-95ee1a64e176"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'happy': 1,\n",
              " 'new': 2,\n",
              " 'years': 3,\n",
              " 'everyone': 4,\n",
              " 'this': 5,\n",
              " 'is': 6,\n",
              " 'a': 7,\n",
              " 'tutorial': 8,\n",
              " 'from': 9,\n",
              " 'me': 10,\n",
              " 'spesial': 11,\n",
              " 'for': 12,\n",
              " 'last': 13,\n",
              " 'night': 14,\n",
              " 'in': 15,\n",
              " '2025': 16,\n",
              " 'i': 17,\n",
              " 'hope': 18,\n",
              " 'will': 19,\n",
              " 'get': 20,\n",
              " 'job': 21,\n",
              " 'and': 22,\n",
              " 'anyone': 23,\n",
              " 'learn': 24,\n",
              " 'can': 25,\n",
              " 'to': 26,\n",
              " 'here': 27,\n",
              " 'find': 28,\n",
              " 'with': 29,\n",
              " 'who': 30,\n",
              " 'ones': 31,\n",
              " 'precious': 32,\n",
              " 'sos': 33,\n",
              " 'eos': 34}"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    }
  ]
}